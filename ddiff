#!/usr/bin/python

import sys
from subprocess import Popen, PIPE
import argparse
import os
import time
from shutil import copyfile



class Ddiff(object):

	def __init__(self):
		pass

		
	def ddiff(self, src_dir, dest_dir):
		# calculate sha1sums of all files (if not already in cache)
		src_sums = sha1sums_cached(src_dir)
		dest_sums = sha1sums_cached(dest_dir)

		new = set()
		modified = set()
		moved = set()
		removed = set()
		dest_hash_dict = {} # {hash:path}
		dest_path_dict = {} # {path:hash}
		
		# parse dest_sums into data structures
		for line in dest_sums.split('\n'):
			# only split on first space to preserve filenames with spaces
			key,path = line.split(' ',1) 
			dest_hash_dict.setdefault(key,[]).append(path)
			dest_path_dict.setdefault(path,[]).append(key)

		# initialize set of dest paths
		# as we categorize them we will remove them from this set
		dest_paths = set(dest_path_dict)
		
		# for path in src
		for line in src_sums.split('\n'):	
			key,path = line.split(' ',1)
			
			# if path is in dest
			if path in dest_path_dict:

				# if hash changed --> modified
				dkey = dest_path_dict[path][0]
				if key != dkey:
					modified.add(path)
					print "mod: {0}".format(joinpath(src_dir,path))
					# this path has been categorized, remove from set
					dest_paths.discard(path)
				
				# else hash same --> no changes 
				else:
					dest_paths.discard(path)
				
			# else path not in dest (moved or new)
			else:
				# if hash is in dest hashes --> moved
				if key in dest_hash_dict:
					dpaths = dest_hash_dict[key]
					
					# if multiple destination paths for this key then duplicates exist
					if len(dpaths) > 1:
						print "dup: {0} {1}".format(joinpath(src_dir, path), str(dpaths))
						new.add(path)
					
					else:
						# there is only one destination path (but it is different from source path)
						moved.add(path)
						dpath = dpaths[0]
						sp = joinpath(dest, src_dir, path)
						dp = joinpath(dest_dir, dpath)
						print "mov: {0} --> {1}".format(dp, sp)
						# this path has been categorized, remove from set
						dest_paths.discard(dpath)

				# else new file (neither hash nor path was in dest)
				else:
					new.add(path)
					print "add: {0}".format(joinpath(src_dir, path))

		# everything that remains in dest were removed / deleted
		removed = dest_paths
		for path in removed:
			#print "removed: [{0}] {1}".format(key, path)
			print "del: {0}".format(joinpath(dest_dir, path))
		
		self.new = new
		self.moved = moved
		self.modified = modified
		self.removed = removed


	def copynew(self, dest_dir):
		dest_dir = realpath(dest_dir)
		if not os.path.isdir(dest_dir):
			raise Exception("destination directory does not exist: " + dest_dir)

		else:
			for path in self.new:
				copy(path, dest_dir)
	
		

			
def sha1sums(path):
	# change directory first so paths are relative to base
	cmd = 'cd {0} && find . -type f -exec sha1sum {{}} \;'.format(path)
	p = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)
	out,err = p.communicate()
	if err:
		sys.stderr.write(err)
	return out.strip()



def joinpath(a, *p):
	"""safer alternative to os.path.join"""
	path = a.rstrip('/')
	for b in p:
		b = b.strip('./ ')
		path += '/' + b
	return path


def realpath(p):
	return os.path.realpath(os.path.expanduser(p))

	
def get_cache_path(target):
	real = realpath(target)
	# get the device ID for this directory
	dev = "{:x}".format(os.stat(real).st_dev)
	fname = real.strip('/').replace('/','.')
	path = joinpath('~/.cache/ddiff/sha1sums', dev, fname)
	path = os.path.expanduser(path)
	return path


def get_from_cache(target, seconds=60*60):
	path = get_cache_path(target)
	if os.path.exists(path):
		mtime = os.path.getmtime(path)
		now = time.time()
		if now-mtime < seconds:
			with open(path) as f:
				return f.read()
	return None


def update_cache(target, data):
	path = get_cache_path(target)
	
	# make sure directory exists
	dpath = os.path.dirname(path)
	if not os.path.isdir(dpath):
		os.makedirs(dpath,0750)
	
	with open(path,'wb') as f:
		f.write(data)
		f.close()
	

def sha1sums_cached(target):
	cached = get_from_cache(target)
	if cached:
		#sys.stderr.write('using cached sha1sums [{0}]\n'.format(get_cache_path(target)))
		return cached
	else:
		out = sha1sums(target)
		update_cache(target, out)
		#sys.stderr.write('caching sha1sums [{0}]\n'.format(get_cache_path(target)))
		return out



if __name__=='__main__':
	parser = argparse.ArgumentParser(description='directory diff')
	parser.add_argument('--diff', metavar=('SOURCE','DEST'), nargs=2, help='diff two directories')
	#parser.add_argument('--copy-new', metavar=('DEST'), nargs=1, action='store_const', const=True, help='')
	parser.add_argument('--verbose', action='store_const', const=True, help='display verbose messages')
	parser.add_argument('--find-duplicates', nargs="*", help='find all duplicate files')
	parser.add_argument('--find-duplicates-interactive', nargs="*", help='find all duplicate files')
	args = parser.parse_args()
	
	# set verbose flag
	global VERBOSE
	VERBOSE = args.verbose if args.verbose else False

	if args.diff:
		print "diffing directories"
		d = Ddiff()
		source,dest = args.diff
		d.ddiff(source, dest)

		#if args.copy_new:
		#	print "copying new files"
		#	d.copynew(args.copy_new)


	elif args.find_duplicates:
		target = args.find_duplicates
		print "Finding all duplicate files (very slow)"
		#find_duplicates(target)

	elif args.find_duplicates_interactive:
		target = args.find_duplicates_interactive
		print "Finding all duplicate files (very slow)"
		#find_duplicates(target, interactive=True)

	# help
	else:
		parser.print_usage()


